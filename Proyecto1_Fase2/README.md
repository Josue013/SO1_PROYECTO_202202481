
---
---
---

<p align="center"> 
<a href="https://git.io/typing-svg"><img src="https://readme-typing-svg.herokuapp.com?font=Fira+Code&size=25&pause=1000&color=00F7E4&center=true&vCenter=true&width=435&lines=*****+PROYECTO+%232+*****" alt="Typing SVG" /></a>
</p>

---
---
---


  <h3 align="center"><strong>Universidad de San Carlos de Guatemala</strong></h3>
  <h3 align="center"><strong>Facultad de IngenierÃ­a</strong></h3>
  <h3 align="center"><strong>Escuela de Ciencias y Sistemas</strong></h3>
  <h3 align="center"><strong>Lab. Sistemas Operativos 1</strong></h3>
  <h3 align="center"><strong>SecciÃ³n: "A"</strong></h3>

---
---
---

<br>
<br>

# Manual TÃ©cnico Fase 2

<br>
<br>

| Nombre                           | Carnet      |
| -------------------------------- | ----------- |
| JosuÃ© NabÃ­ Hurtarte Pinto        | `202202481` |

---

## <a name="indice">ğŸ“… INDICE

|     | Titulo                      | Link         |
| --- | --------------------------- | ------------ |
| 1   | `Descripcion del proyecto`  | [IR](#desc)  |
| 2   | `Arquitectura del proyecto` | [IR](#arq)   |
| 3   | `Modulo de procesos`        | [IR](#pro)   |
| 4   | `Locust`                    | [IR](#loc)   |
| 5   | `Kubernetes`                | [IR](#kub)   |
| 6   | `Rutas`                     | [IR](#ru)    |
| 7   | `Socket.io`                 | [IR](#sktio) |
| 8   | `Cloud Sql`                 | [IR](#csql)  |
| 9   | `Cloud Run`                 | [IR](#crun)  |

---

## <a name="desc">1. DescripciÃ³n del proyecto

### ğŸ“‹ Objetivo General

La **Fase 2** del proyecto Sistema de Monitoreo de Recursos evoluciona la arquitectura hacia una **soluciÃ³n cloud-native completa**, implementando conceptos avanzados de DevOps, orquestaciÃ³n de contenedores, microservicios y monitoreo en tiempo real. El objetivo es crear una infraestructura escalable y robusta utilizando Google Cloud Platform y tecnologÃ­as modernas de contenedores.

### ğŸ¯ EvoluciÃ³n respecto a Fase 1

| **Aspecto**              | **Fase 1**                    | **Fase 2**                           |
|--------------------------|-------------------------------|--------------------------------------|
| **Infraestructura**     | Docker Compose local          | Google Kubernetes Engine (GKE)      |
| **Base de Datos**       | MySQL en contenedor           | Cloud SQL (MySQL administrado)      |
| **Escalabilidad**       | MonolÃ­tico                     | Microservicios distribuidos         |
| **Balanceador**         | Sin balanceador                | NGINX + Traffic Split 50/50         |
| **Tiempo Real**         | Polling cada segundo           | WebSockets con Socket.io            |
| **Frontend**            | React bÃ¡sico                   | React + Cloud Run + Tiempo Real     |
| **CI/CD**               | Manual                         | GitHub Actions automatizado         |
| **Monitoreo**           | Logs bÃ¡sicos                   | Observabilidad completa              |

### ğŸš€ Funcionalidades Nuevas

#### **ğŸŒ Infraestructura Cloud-Native:**
- **Google Kubernetes Engine**: OrquestaciÃ³n automÃ¡tica de contenedores
- **Cloud SQL**: Base de datos MySQL totalmente administrada
- **Cloud Run**: Despliegue serverless para frontend
- **Load Balancer**: DistribuciÃ³n inteligente de trÃ¡fico

#### **âš–ï¸ Balanceador de Carga Avanzado:**
- **Traffic Split 50/50**: DistribuciÃ³n equitativa entre APIs de Python y Node.js
- **NGINX Ingress**: ConfiguraciÃ³n de rutas y balanceo automÃ¡tico
- **Health Checks**: VerificaciÃ³n automÃ¡tica de estado de servicios
- **Failover**: RedirecciÃ³n automÃ¡tica en caso de fallos

#### **ğŸ“¡ Tiempo Real con WebSockets:**
- **Socket.io**: ComunicaciÃ³n bidireccional en tiempo real
- **API Node.js dedicada**: Servicio especializado para WebSockets
- **Frontend reactivo**: Actualizaciones instantÃ¡neas sin polling
- **Escalabilidad horizontal**: MÃºltiples instancias de APIs

#### **ğŸ“Š Observabilidad Completa:**
- **Logs centralizados**: AgregaciÃ³n de logs de todos los servicios
- **MÃ©tricas de sistema**: CPU, RAM, procesos en tiempo real
- **Health endpoints**: Monitoreo de estado de cada microservicio
- **Alertas automÃ¡ticas**: Notificaciones en caso de fallos

### ğŸ—ï¸ Arquitectura de Microservicios

La Fase 2 implementa una **arquitectura de microservicios distribuida**:

1. **VM Linux**: Sistema base con mÃ³dulos de kernel para recolecciÃ³n de mÃ©tricas
2. **Cluster GKE**: OrquestaciÃ³n de 4 microservicios independientes
3. **Load Balancer**: NGINX que distribuye carga entre APIs
4. **APIs duales**: Python (FastAPI) + Node.js (Express) con traffic split
5. **Tiempo Real**: API Node.js especializada con Socket.io
6. **Frontend**: React en Cloud Run con WebSockets
7. **Base de Datos**: Cloud SQL MySQL con alta disponibilidad

### ğŸ“ Conceptos Implementados

- **OrquestaciÃ³n de Contenedores**: Kubernetes en producciÃ³n
- **Microservicios**: Servicios independientes y escalables
- **Load Balancing**: DistribuciÃ³n inteligente de carga
- **Tiempo Real**: WebSockets para comunicaciÃ³n instantÃ¡nea
- **Cloud Computing**: Servicios administrados de Google Cloud
- **Observabilidad**: Monitoreo y logging distribuido

---

## <a name="arq">2. Arquitectura del proyecto

![arquitectura](./imgs/image.png)

### ğŸ”§ Componentes Principales

| **Componente**     | **TecnologÃ­a**      | **FunciÃ³n**                             |
| ------------------ | ------------------- | --------------------------------------- |
| Maquina Virtual    | Ubuntu 22.04        | Sistema con mÃ³dulos kernel              |
| Locust             | Python + Locust     | GeneraciÃ³n de carga y pruebas de estrÃ©s |
| Kubernetes Cluster | GKE                 | OrquestaciÃ³n de microservicios          |
| Load Balancer      | NGINX Ingress       | Traffic split y balanceo                |
| Python API         | Flask               | Procesamiento de mÃ©tricas               |
| Node.js API        | Express             | API REST alternativa                    |
| Realtime API       | Node.js + Socket.io | WebSockets en tiempo real               |
| Frontend           | React + Cloud Run   | Interfaz de usuario                     |
| Database           | Cloud SQL MySQL     | Almacenamiento persistente              |

---

## <a name="pro">3. MÃ³dulo de procesos

### ğŸ“‹ Objetivo del mÃ³dulo
Proporcionar informaciÃ³n detallada sobre los procesos del sistema operativo en tiempo real, incluyendo conteos por estado (corriendo, durmiendo, zombies, parados) y total de procesos. El mÃ³dulo crea una interfaz en el sistema de archivos `/proc` que permite el acceso directo a las mÃ©tricas de procesos desde el espacio de usuario.

### âš™ï¸ Funcionalidad del mÃ³dulo

El mÃ³dulo `procesos_202202481` implementa las siguientes caracterÃ­sticas:

- **IteraciÃ³n completa de procesos**: Utiliza `for_each_process(task)` para recorrer todos los procesos del sistema
- **AnÃ¡lisis de estados**: Examina el campo `task->__state` de cada proceso para clasificarlos por estado
- **DetecciÃ³n de zombies**: Verifica el campo `task->exit_state` para identificar procesos zombie
- **Conteo automÃ¡tico**: Calcula automÃ¡ticamente:
  - Total de procesos en el sistema
  - Procesos en estado corriendo (`TASK_RUNNING`)
  - Procesos durmiendo (`TASK_INTERRUPTIBLE` | `TASK_UNINTERRUPTIBLE`)
  - Procesos parados (`__TASK_STOPPED` | `__TASK_TRACED`)
  - Procesos zombie (`EXIT_ZOMBIE`)
- **Formato JSON**: Exporta toda la informaciÃ³n en formato JSON estructurado para fÃ¡cil consumo por aplicaciones
- **Interfaz `/proc`**: Crea el archivo `/proc/procesos_202202481` para acceso desde espacio de usuario

### ğŸ—ï¸ Algoritmo de clasificaciÃ³n

El mÃ³dulo implementa un algoritmo de clasificaciÃ³n basado en las constantes del kernel:

1. **IteraciÃ³n de procesos**: Recorre la lista de procesos del kernel usando `for_each_process()`
2. **AnÃ¡lisis de estado principal**: Examina `task->__state` para determinar el estado actual
3. **ClasificaciÃ³n por estado**:
   - `TASK_RUNNING (0x00000000)`: Proceso ejecutÃ¡ndose o listo para ejecutar
   - `TASK_INTERRUPTIBLE (0x00000001)`: Proceso durmiendo, puede ser interrumpido por seÃ±ales
   - `TASK_UNINTERRUPTIBLE (0x00000002)`: Proceso durmiendo, no puede ser interrumpido
   - `__TASK_STOPPED (0x00000004)`: Proceso parado por seÃ±al
   - `__TASK_TRACED (0x00000008)`: Proceso siendo rastreado por debugger
4. **DetecciÃ³n de zombies**: Verifica `task->exit_state & EXIT_ZOMBIE` independientemente del estado principal
5. **Conteo incremental**: Mantiene contadores separados para cada categorÃ­a

### ğŸ“Š Estados de procesos monitoreados

| **Estado**              | **Constante del Kernel**    | **DescripciÃ³n**                           |
|-------------------------|----------------------------|-------------------------------------------|
| **Corriendo**           | `TASK_RUNNING`             | Proceso ejecutÃ¡ndose o en cola de CPU    |
| **Durmiendo**           | `TASK_INTERRUPTIBLE`       | Esperando evento, puede ser interrumpido |
| **Durmiendo profundo**  | `TASK_UNINTERRUPTIBLE`     | Esperando I/O, no puede ser interrumpido |
| **Parado**              | `__TASK_STOPPED`           | Detenido por seÃ±al (SIGSTOP, SIGTSTP)    |
| **Rastreado**           | `__TASK_TRACED`            | Bajo control de debugger (gdb, strace)   |
| **Zombie**              | `EXIT_ZOMBIE`              | Proceso terminado esperando recolecciÃ³n   |

### ğŸ’» CÃ³digo del mÃ³dulo

```c
#include <linux/module.h>
#include <linux/kernel.h>
#include <linux/init.h>
#include <linux/proc_fs.h>
#include <linux/seq_file.h>
#include <linux/sched.h>
#include <linux/sched/signal.h>

MODULE_LICENSE("GPL");
MODULE_AUTHOR("Josue Nabi Hurtarte Pinto");
MODULE_DESCRIPTION("Sistemas Operativos 1 - Proyecto - Fase 2");
MODULE_VERSION("1.0");

/*
Estructura del archivo /proc/procesos_202202481
{
  "procesos_corriendo": 123,
  "total_procesos": 233,
  "procesos_durmiendo": 65,
  "procesos_zombies": 65,
  "procesos_parados": 65
}
*/

// Estados de procesos del kernel
#define TASK_RUNNING            0x00000000
#define TASK_INTERRUPTIBLE      0x00000001
#define TASK_UNINTERRUPTIBLE    0x00000002
#define __TASK_STOPPED          0x00000004
#define __TASK_TRACED           0x00000008
// Estados de salida
#define EXIT_DEAD               0x00000010
#define EXIT_ZOMBIE             0x00000020

// FunciÃ³n que cuenta los procesos por estado
static void contar_procesos(int *corriendo, int *total, int *durmiendo, int *zombies, int *parados) {
    struct task_struct *task;
    
    // Inicializar contadores
    *corriendo = 0;
    *total = 0;
    *durmiendo = 0;
    *zombies = 0;
    *parados = 0;

    // Iterar sobre todos los procesos del sistema
    for_each_process(task) {
        (*total)++;
        
        // Verificar el estado del proceso
        if (task->__state == TASK_RUNNING) {
            (*corriendo)++;
        }
        else if (task->__state & (TASK_INTERRUPTIBLE | TASK_UNINTERRUPTIBLE)) {
            (*durmiendo)++;
        }
        else if (task->__state & (__TASK_STOPPED | __TASK_TRACED)) {
            (*parados)++;
        }
        
        // Verificar si es zombie
        if (task->exit_state & EXIT_ZOMBIE) {
            (*zombies)++;
        }
    }
}

// FunciÃ³n que se llama cuando se lee el archivo /proc/procesos_202202481
static int mostrar_procesos(struct seq_file *archivo, void *v) {
    int procesos_corriendo, total_procesos, procesos_durmiendo, procesos_zombies, procesos_parados;
    
    // Obtener conteos de procesos
    contar_procesos(&procesos_corriendo, &total_procesos, &procesos_durmiendo, &procesos_zombies, &procesos_parados);
    
    // Generar salida en formato JSON
    seq_printf(archivo, "{\n");
    seq_printf(archivo, "  \"procesos_corriendo\": %d,\n", procesos_corriendo);
    seq_printf(archivo, "  \"total_procesos\": %d,\n", total_procesos);
    seq_printf(archivo, "  \"procesos_durmiendo\": %d,\n", procesos_durmiendo);
    seq_printf(archivo, "  \"procesos_zombies\": %d,\n", procesos_zombies);
    seq_printf(archivo, "  \"procesos_parados\": %d\n", procesos_parados);
    seq_printf(archivo, "}\n");
    
    return 0;
}

// Cuando se le hace un cat al mÃ³dulo
static int abrir_proc(struct inode *inode, struct file *file) {
    return single_open(file, mostrar_procesos, NULL);
}

// Estructura de operaciones del archivo /proc/procesos_202202481
static const struct proc_ops procesos_ops = {
    .proc_open = abrir_proc,
    .proc_read = seq_read
};

// FunciÃ³n de inicializaciÃ³n del mÃ³dulo
static int __init _insert(void) {
    proc_create("procesos_202202481", 0444, NULL, &procesos_ops);
    printk(KERN_INFO "Modulo Procesos cargado: /proc/procesos_202202481 creado\n");
    return 0;
}

// FunciÃ³n de limpieza del mÃ³dulo
static void __exit _delete(void) {
    remove_proc_entry("procesos_202202481", NULL);
    printk(KERN_INFO "Modulo Procesos descargado: /proc/procesos_202202481 eliminado\n");
}

module_init(_insert);
module_exit(_delete);
```

### CompilaciÃ³n e instalaciÃ³n del mÃ³dulo

makefile:

```makefile
obj-m += procesos_202202481.o

all:
    make -C /lib/modules/$(shell uname -r)/build M=$(PWD) modules

clean:
    make -C /lib/modules/$(shell uname -r)/build M=$(PWD) clean
```

CompilaciÃ³n:

```bash
# Navegar al directorio del mÃ³dulo
cd Kernel/process/

# Limpiar compilaciones anteriores
make clean

# Compilar el mÃ³dulo
make all
```

Instalacion:
```bash
# Cargar el mÃ³dulo (requiere sudo)
sudo insmod procesos_202202481.ko

# Verificar que se cargÃ³ correctamente
lsmod | grep procesos_202202481

# Verificar que el archivo /proc fue creado
ls -la /proc/procesos_202202481
```

## <a name="loc">4. Locust 

### ğŸ“‹ Objetivo de Locust en Fase 2

Locust en la Fase 2 implementa un **sistema de recolecciÃ³n y distribuciÃ³n de carga en dos etapas**, diseÃ±ado para probar tanto la capacidad de la VM como el traffic splitting del ingress. El objetivo es simular un flujo real de datos desde una fuente (VM) hacia un sistema distribuido (Kubernetes cluster).

### ğŸš€ Flujo de trabajo de Locust 

![locust_flow](./imgs/Locust.png)

### ğŸ“„ Archivos de Locust

#### **1. traffic_collector.py - Recolector de datos de VM**

##### **ğŸ¯ Objetivo:**
Simular mÃºltiples usuarios recolectando datos de la mÃ¡quina virtual para generar carga y obtener mÃ©tricas reales del sistema bajo estrÃ©s. Este archivo actÃºa como **cliente de la VM** y **generador de datos**.

##### **âš™ï¸ Funcionalidades principales:**

- **GeneraciÃ³n de carga**: Simula 300+ usuarios concurrentes accediendo a la VM
- **RecolecciÃ³n automÃ¡tica**: Obtiene mÃ©tricas de CPU, RAM y procesos en tiempo real
- **Almacenamiento estructurado**: Guarda datos en formato JSON para posterior procesamiento
- **Control de calidad**: Valida que los datos obtenidos sean completos y Ãºtiles
- **LÃ­mite inteligente**: Se detiene automÃ¡ticamente al alcanzar el objetivo de registros

##### **ğŸ”§ ConfiguraciÃ³n recomendada:**
```bash
locust -f traffic_collector.py --host=http://VM_IP:5200 \
       --users 300 --spawn-rate 1 --run-time 180s --headless
```

#### **2. traffic_splitter.py - Enviador de trafico**

##### **ğŸ¯ Objetivo:**

Probar el balanceador de carga NGINX enviando los datos recolectado al ingress del cluster kubernetes. Este archivo actua como cliente del cluster y validador del traffic splitting.  

##### **âš™ï¸ Funcionalidades principales:**

- **Datos Ãºnicos**: EnvÃ­a datos Ãºnicos para evitar duplicados en el sistema
- **Monitoreo de distribuciÃ³n**: Rastrea que API procesÃ³ cada peticion.
- **Estadisticas en tiempo real**: Muestra progreso y distribucion cada 50 registros.

##### **ğŸ”§ ConfiguraciÃ³n recomendada:**
```bash
locust -f send_to_ingress.py --host=http://INGRESS_IP \
       --users 150 --spawn-rate 1 --run-time 180s --headless
```

## <a name="kub">5. Kubernetes

### ğŸ“‹ Objetivo de Kubernetes en Fase 2

Google Kubernetes Engine (GKE) actÃºa como el **nÃºcleo de orquestaciÃ³n** de la infraestructura cloud-native, administrando automÃ¡ticamente el despliegue, escalado y balanceo de carga de los microservicios. El cluster implementa una **arquitectura de servicios distribuidos** con alta disponibilidad y escalabilidad horizontal.

### ğŸ—ï¸ Arquitectura del Cluster GKE

```
Google Kubernetes Engine (GKE)
â”œâ”€â”€ Namespace: sopes1-fase2
â”œâ”€â”€ Deployment: python-api-deployment (1 replica)
â”œâ”€â”€ Deployment: nodejs-api-deployment (1 replica)  
â”œâ”€â”€ Deployment: nodejs-realtime-api-deployment (1 replica)
â”œâ”€â”€ Deployment: metrics-proxy (NGINX - 1 replica)
â”œâ”€â”€ Services: ClusterIP + NodePort
â””â”€â”€ Ingress: External Load Balancer
```

### ğŸ›ï¸ Namespace: sopes1-fase2

#### **ğŸ¯ PropÃ³sito:**
Crear un **entorno aislado** dentro del cluster para todos los recursos del proyecto, proporcionando separaciÃ³n lÃ³gica y control de acceso granular.

#### **ğŸ“‹ Recursos contenidos:**
- **4 Deployments**: APIs Python, Node.js, Socket.io y NGINX proxy
- **4 Services**: Servicios internos para comunicaciÃ³n entre pods
- **1 Ingress**: Load balancer externo para acceso pÃºblico
- **1 ConfigMap**: ConfiguraciÃ³n de NGINX para traffic splitting

#### **ğŸ”§ Ventajas del namespace:**
- **Aislamiento**: SeparaciÃ³n completa de otros proyectos
- **GestiÃ³n simplificada**: Operaciones agrupadas por namespace
- **Control de recursos**: LÃ­mites de CPU/RAM por namespace
- **Seguridad**: PolÃ­ticas de red especÃ­ficas

### ğŸ³ Deployments (Pods)

#### **1. python-api-deployment**

##### **ğŸ¯ FunciÃ³n:**
Despliegue de la **API Python Flask** para procesamiento de mÃ©tricas y almacenamiento en Cloud SQL.

##### **âš™ï¸ ConfiguraciÃ³n clave:**
- **Imagen**: `josue013/sopes1-python-api:latest`
- **Puerto interno**: 5000
- **RÃ©plicas**: 1 (escalable horizontalmente)
- **Recursos**: 256Mi RAM, 0.25 CPU (lÃ­mites: 512Mi, 0.5 CPU)
- **Health checks**: Liveness + Readiness probes en `/health`

##### **ğŸ”— Variables de entorno:**
- ConexiÃ³n directa a Cloud SQL MySQL
- Credenciales de base de datos seguras
- ConfiguraciÃ³n de puerto personalizable

---

#### **2. nodejs-api-deployment**

##### **ğŸ¯ FunciÃ³n:**
Despliegue de la **API Node.js Express** como alternativa ligera para procesamiento rÃ¡pido de mÃ©tricas y balanceo de carga con Python.

##### **âš™ï¸ ConfiguraciÃ³n clave:**
- **Imagen**: `josue013/sopes1-nodejs-api:latest`
- **Puerto interno**: 3000
- **RÃ©plicas**: 1 (escalable horizontalmente)
- **Recursos**: 256Mi RAM, 0.25 CPU (lÃ­mites: 512Mi, 0.5 CPU)
- **Health checks**: Liveness + Readiness probes en `/health`

##### **ğŸ”— Variables de entorno:**
- Misma base de datos que Python API (Cloud SQL)
- ConfiguraciÃ³n idÃ©ntica para traffic splitting
- Puerto configurable via variable de entorno

---

#### **3. nodejs-realtime-api-deployment**

##### **ğŸ¯ FunciÃ³n:**
Despliegue **especializado en tiempo real** con Socket.io para comunicaciÃ³n bidireccional WebSocket con el frontend.

##### **âš™ï¸ ConfiguraciÃ³n clave:**
- **Imagen**: `josue013/sopes1-nodejs-realtime-api:latest`
- **Puerto interno**: 4000
- **RÃ©plicas**: 1 (optimizado para WebSocket persistence)
- **Recursos**: 256Mi RAM, 0.25 CPU (lÃ­mites: 512Mi, 0.5 CPU)
- **Health checks**: VerificaciÃ³n de Socket.io endpoint

##### **ğŸ”— CaracterÃ­sticas especiales:**
- **WebSocket support**: Configurado para Socket.io
- **CORS habilitado**: Para frontend en Cloud Run
- **ConexiÃ³n persistente**: Mantiene estado de conexiones activas

---

#### **4. metrics-proxy (NGINX)**

##### **ğŸ¯ FunciÃ³n:**
**Load balancer interno** que implementa traffic splitting 50/50 entre las APIs Python y Node.js, actuando como punto de entrada Ãºnico.

##### **âš™ï¸ ConfiguraciÃ³n clave:**
- **Imagen**: `nginx:alpine`
- **Puerto interno**: 80
- **RÃ©plicas**: 1 (punto Ãºnico de entrada)
- **ConfigMap**: ConfiguraciÃ³n NGINX personalizada
- **Upstream**: Round-robin entre python-api y nodejs-api

##### **ğŸ”— Rutas configuradas:**
- `/health` â†’ Python API (health check principal)
- `/metrics` â†’ Round-robin 50/50 (traffic splitting)
- `/debug` â†’ InformaciÃ³n del balanceador
- `/` â†’ Status pÃ¡gina del proxy

### ğŸŒ Services (ComunicaciÃ³n Interna)

#### **Tipos de Services implementados:**

##### **1. ClusterIP Services (ComunicaciÃ³n Interna)**

###### **python-api-service:**
- **Tipo**: ClusterIP (interno al cluster)
- **Puerto**: 80 â†’ 5000 (pod)
- **FunciÃ³n**: Exponer Python API internamente
- **Acceso**: Solo desde otros pods del cluster

###### **nodejs-api-service:**
- **Tipo**: ClusterIP (interno al cluster)
- **Puerto**: 80 â†’ 3000 (pod)
- **FunciÃ³n**: Exponer Node.js API internamente
- **Acceso**: Solo desde otros pods del cluster

###### **metrics-proxy-service:**
- **Tipo**: ClusterIP (interno al cluster)
- **Puerto**: 80 â†’ 80 (nginx pod)
- **FunciÃ³n**: Exponer NGINX proxy internamente
- **Acceso**: Recibe trÃ¡fico del Ingress

##### **2. NodePort Service (Acceso Externo)**

###### **nodejs-realtime-api-service:**
- **Tipo**: NodePort (acceso externo directo)
- **Puerto**: 80 â†’ 4000 (pod)
- **NodePort**: 30080 (puerto fijo en nodos)
- **FunciÃ³n**: Acceso directo para WebSockets
- **Motivo**: Bypass del Ingress para Socket.io

### ğŸ“Š Flujo de comunicaciÃ³n

#### **TrÃ¡fico interno (APIs REST):**
```
Ingress â†’ metrics-proxy-service â†’ NGINX Pod
   â†“
NGINX (Round-robin) 
   â”œâ”€â”€ python-api-service â†’ Python Pod
   â””â”€â”€ nodejs-api-service â†’ Node.js Pod
```

#### **TrÃ¡fico WebSocket (Tiempo Real):**
```
Frontend â†’ NodeIP:30080 â†’ nodejs-realtime-api-service â†’ Socket.io Pod
```

### ğŸ”§ Comandos de despliegue

#### **CreaciÃ³n del cluster:**
```bash
# Crear cluster GKE optimizado
gcloud container clusters create sopes1-cluster \
  --zone=us-central1-a \
  --num-nodes=1 \
  --machine-type=n1-standard-2 \
  --enable-autoscaling \
  --max-nodes=3
```

#### **ConfiguraciÃ³n del namespace:**
```bash
# Crear namespace dedicado
kubectl create namespace sopes1-fase2

# Establecer como namespace por defecto
kubectl config set-context --current --namespace=sopes1-fase2
```

#### **Despliegue de servicios:**
```bash
# Desplegar Python API
kubectl apply -f python.yaml

# Desplegar Node.js API
kubectl apply -f nodejs1.yaml

# Desplegar Socket.io API
kubectl apply -f nodejs2.yaml

# Desplegar NGINX Proxy + Ingress
kubectl apply -f ingress.yaml
```

#### **VerificaciÃ³n del despliegue:**
```bash
# Verificar pods
kubectl get pods -n sopes1-fase2

# Verificar services
kubectl get services -n sopes1-fase2

# Verificar logs
kubectl logs deployment/python-api-deployment -n sopes1-fase2
```

### ğŸ¯ Ventajas de la implementaciÃ³n en Kubernetes

- **OrquestaciÃ³n automÃ¡tica**: GestiÃ³n de ciclo de vida de contenedores
- **Service discovery**: ComunicaciÃ³n automÃ¡tica entre servicios
- **Load balancing**: DistribuciÃ³n inteligente de carga
- **Health checks**: VerificaciÃ³n automÃ¡tica de estado de servicios
- **Rolling updates**: Actualizaciones sin downtime
- **Resource management**: Control granular de CPU y memoria
- **Namespace isolation**: SeparaciÃ³n lÃ³gica de recursos
- **Horizontal scaling**: Escalado automÃ¡tico bajo demanda

### ğŸ” Monitoreo y debugging

#### **Estado del cluster:**
```bash
# Estado general
kubectl get all -n sopes1-fase2

# DescripciÃ³n detallada de pods
kubectl describe pod <pod-name> -n sopes1-fase2

# Logs en tiempo real
kubectl logs -f deployment/python-api-deployment -n sopes1-fase2
```

#### **MÃ©tricas de recursos:**
```bash
# Uso de recursos por pod
kubectl top pods -n sopes1-fase2

# Uso de recursos por nodo
kubectl top nodes
```

La implementaciÃ³n de Kubernetes en Fase 2 transforma la arquitectura de un sistema monolÃ­tico hacia una **soluciÃ³n cloud-native distribuida**, proporcionando la base sÃ³lida para escalabilidad, alta disponibilidad y gestiÃ³n automatizada de la infraestructura.

## <a name="ru">6. Rutas

### ğŸ“‹ Objetivo de las Rutas en Fase 2

El sistema implementa **dos rutas independientes** para el procesamiento y almacenamiento de mÃ©tricas del sistema, permitiendo **evaluaciÃ³n comparativa de rendimiento** entre tecnologÃ­as Python y Node.js. Ambas rutas utilizan **comunicaciÃ³n RPC** (Remote Procedure Call) para registrar datos eficientemente en Cloud SQL MySQL, implementando un **traffic splitting 50/50** a travÃ©s del load balancer NGINX.

### ğŸ—ï¸ Arquitectura de Traffic Splitting

```
Locust/VM â†’ Ingress (Load Balancer) â†’ NGINX Proxy
                                          â†“
                                    Round-robin 50/50
                                          â†“
                              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                              â†“                      â†“
                        Ruta 1 (Python)      Ruta 2 (Node.js)
                         Flask API            Express API
                              â†“                      â†“
                        Cloud SQL MySQL â† â†’ Cloud SQL MySQL
                        (Misma instancia)     (Misma instancia)
```

### ğŸ›£ï¸ Ruta 1: API Python Flask

#### **ğŸ¯ Objetivo:**
Implementar un **servicio REST robusto** utilizando Python Flask para recibir mÃ©tricas del sistema y almacenarlas eficientemente en Cloud SQL MySQL. Esta ruta se enfoca en **procesamiento confiable** y **manejo avanzado de errores**.

#### **âš™ï¸ CaracterÃ­sticas principales:**

##### **ğŸ”§ TecnologÃ­as utilizadas:**
- **Framework**: Flask (Python)
- **Base de datos**: PyMySQL (conector MySQL nativo)
- **CORS**: Flask-CORS para comunicaciÃ³n cross-origin
- **Variables de entorno**: python-dotenv para configuraciÃ³n segura
- **Logging**: Sistema de logs detallado con emojis para debugging

##### **ğŸ“Š Endpoints implementados:**

###### **`GET /health` - Health Check**
- **FunciÃ³n**: VerificaciÃ³n de estado del servicio
- **Respuesta**: JSON con timestamp y estado del servicio
- **Uso**: Kubernetes health probes y monitoreo

###### **`POST /metrics` - Procesamiento de mÃ©tricas**
- **FunciÃ³n**: Recibir y procesar mÃ©tricas del sistema
- **Entrada**: JSON con datos de CPU, RAM y procesos
- **Procesamiento**: 
  - ValidaciÃ³n de estructura JSON
  - Soporte para registros Ãºnicos o arrays
  - InserciÃ³n en Cloud SQL con manejo de errores
- **Respuesta**: ConfirmaciÃ³n de registros procesados con identificaciÃ³n de API

##### **ğŸ—„ï¸ Manejo de base de datos:**
```python
# ConfiguraciÃ³n segura con variables de entorno
DB_CONFIG = {
    'host': os.getenv('DB_HOST'),
    'user': os.getenv('DB_USER'),
    'password': os.getenv('DB_PASSWORD'),
    'database': os.getenv('DB_NAME'),
    'port': int(os.getenv('DB_PORT')),
    'charset': 'utf8mb4'
}
```

##### **ğŸ“ Estructura de datos procesados:**
- **CPU**: Porcentaje de uso y porcentaje libre
- **RAM**: Total, usado, libre y porcentaje de uso
- **Procesos**: Conteos por estado (corriendo, durmiendo, zombie, parados)
- **Metadatos**: Timestamp automÃ¡tico y origen de datos (`python_api`)

##### **ğŸ” Logging y monitoreo:**
```
ğŸ [PYTHON API] Datos recibidos: 1 registros
âœ… [PYTHON API] CPU: 45%, RAM: 67%, Procesos: 156
ğŸ [PYTHON API] Registro insertado exitosamente
```

---

### ğŸ›£ï¸ Ruta 2: API Node.js Express

#### **ğŸ¯ Objetivo:**
Implementar un **servicio REST de alto rendimiento** utilizando Node.js Express para recibir mÃ©tricas del sistema y almacenarlas eficientemente en Cloud SQL MySQL. Esta ruta se enfoca en **velocidad de procesamiento** y **escalabilidad horizontal**.

#### **âš™ï¸ CaracterÃ­sticas principales:**

##### **ğŸ”§ TecnologÃ­as utilizadas:**
- **Framework**: Express.js (Node.js)
- **Base de datos**: mysql2/promise (conector moderno con soporte async/await)
- **CORS**: CORS middleware nativo
- **Variables de entorno**: dotenv para configuraciÃ³n
- **Connection pooling**: Pool de conexiones para optimizaciÃ³n

##### **ğŸ“Š Endpoints implementados:**

###### **`GET /health` - Health Check**
- **FunciÃ³n**: VerificaciÃ³n de estado del servicio
- **Respuesta**: JSON con timestamp y identificaciÃ³n del servicio
- **Uso**: Kubernetes health probes y balanceador de carga

###### **`POST /metrics` - Procesamiento de mÃ©tricas**
- **FunciÃ³n**: Recibir y procesar mÃ©tricas del sistema con alta performance
- **Entrada**: JSON con datos de CPU, RAM y procesos
- **Procesamiento**:
  - ValidaciÃ³n asÃ­ncrona de datos
  - Soporte para registros Ãºnicos o arrays masivos
  - InserciÃ³n optimizada con connection pooling
- **Respuesta**: ConfirmaciÃ³n de registros procesados con identificaciÃ³n de API

##### **ğŸ—„ï¸ OptimizaciÃ³n de base de datos:**
```javascript
// Connection pool para alto rendimiento
const pool = mysql.createPool({
  host: process.env.DB_HOST,
  user: process.env.DB_USER,
  password: process.env.DB_PASSWORD,
  database: process.env.DB_NAME,
  port: parseInt(process.env.DB_PORT),
  connectionLimit: 10,
  acquireTimeout: 60000,
  timeout: 60000,
});
```

##### **ğŸ“ Procesamiento asÃ­ncrono:**
- **Async/Await**: Manejo moderno de operaciones asÃ­ncronas
- **Connection pooling**: ReutilizaciÃ³n eficiente de conexiones
- **Batch processing**: Soporte para inserciÃ³n masiva de registros
- **Error handling**: GestiÃ³n robusta de errores con rollback automÃ¡tico

##### **ğŸ” Logging y monitoreo:**
```
ğŸŸ¢ [NODE API 1] Datos recibidos: 1 registros
ğŸŸ¢ [NODE API 1] CPU: 45%, RAM: 67%, Procesos: 156
âœ… [NODE API 1] Insertados 1/1 registros
```

---

### ğŸ”„ Traffic Splitting Implementation

#### **ğŸ“ˆ DistribuciÃ³n automÃ¡tica 50/50:**

El load balancer NGINX implementa **round-robin perfecto** entre las dos rutas:

```nginx
upstream backend_pool {
    server python-api-service:80 weight=1;
    server nodejs-api-service:80 weight=1;
}

location /metrics {
    proxy_pass http://backend_pool/metrics;
    # ConfiguraciÃ³n de balanceo automÃ¡tico
}
```

#### **ğŸ“Š Monitoreo de distribuciÃ³n:**

Locust verifica que el traffic splitting funcione correctamente:

```
ğŸ“Š Enviados: 1000/2000 (50.0%) | Python: 501 (50.1%) | Node.js: 499 (49.9%)
âœ… Traffic splitting funcionando correctamente dentro del margen Â±2%
```

### ğŸ”§ ConfiguraciÃ³n de despliegue

#### **Variables de entorno compartidas:**
```bash
DB_HOST=34.61.65.213          # Cloud SQL IP privada
DB_USER=root                  # Usuario de base de datos
DB_PASSWORD=8AraHXK#EJL4\Fmq  # Password seguro
DB_NAME=sopes1_db            # Base de datos del proyecto
DB_PORT=3306                 # Puerto MySQL estÃ¡ndar
```

#### **Comandos de despliegue:**
```bash
# Desplegar ambas rutas
kubectl apply -f python.yaml   # Ruta 1: Python Flask
kubectl apply -f nodejs1.yaml  # Ruta 2: Node.js Express

# Verificar funcionamiento
kubectl get pods -n sopes1-fase2
kubectl logs deployment/python-api-deployment -n sopes1-fase2
kubectl logs deployment/nodejs-api-deployment -n sopes1-fase2
```

### ğŸ¯ Resultados esperados

#### **Funcionalidad idÃ©ntica:**
- âœ… Ambas rutas procesan los mismos datos de entrada
- âœ… Ambas almacenan en la misma tabla de Cloud SQL
- âœ… Ambas responden con el mismo formato JSON
- âœ… Ambas implementan health checks compatibles

#### **Diferencias de implementaciÃ³n:**
- **Python**: Enfoque en robustez y debugging
- **Node.js**: Enfoque en performance y escalabilidad
- **Traffic Split**: DistribuciÃ³n equitativa automÃ¡tica
- **Observabilidad**: Logs diferenciados por tecnologÃ­a


La implementaciÃ³n de rutas duales permite **optimizaciÃ³n hÃ­brida**, donde el sistema puede aprovechar las fortalezas de cada tecnologÃ­a segÃºn los patrones de carga especÃ­ficos, mientras que el traffic splitting garantiza **distribuciÃ³n equitativa** y **evaluaciÃ³n comparativa objetiva** del rendimiento.

## <a name="sktio">7. Socket.io

### ğŸ“‹ Objetivo de Socket.io en Fase 2

Socket.io implementa un **sistema de comunicaciÃ³n en tiempo real bidireccional** entre el cluster Kubernetes y el frontend desplegado en Cloud Run. El objetivo es proporcionar **actualizaciones instantÃ¡neas** de mÃ©tricas del sistema sin necesidad de polling, creando una experiencia de usuario **reactiva y eficiente** con latencia mÃ­nima.

### ğŸ—ï¸ Arquitectura de Tiempo Real

```
VM (Datos) â†’ APIs (Python/Node.js) â†’ Cloud SQL â†’ Socket.io API â†’ WebSocket â†’ Frontend (Cloud Run)
     â†“              â†“                    â†“            â†“              â†“           â†“
  MÃ©tricas    Almacenamiento        Persistencia   Tiempo Real    WSS/WS     VisualizaciÃ³n
```

### ğŸš€ Flujo de ComunicaciÃ³n WebSocket

#### **ğŸ“¡ Proceso de conexiÃ³n:**
1. **Frontend se conecta** â†’ Socket.io server via NodePort (30080)
2. **Handshake inicial** â†’ VerificaciÃ³n de CORS y autenticaciÃ³n
3. **Datos inmediatos** â†’ EnvÃ­o de Ãºltimo registro de Cloud SQL
4. **ConexiÃ³n persistente** â†’ Canal bidireccional establecido
5. **Actualizaciones automÃ¡ticas** â†’ Broadcast cada 2 segundos

### âš™ï¸ API Node.js Socket.io Especializada

#### **ğŸ¯ FunciÃ³n principal:**
Servicio **dedicado exclusivamente** a WebSockets que actÃºa como **puente en tiempo real** entre Cloud SQL y el frontend, proporcionando mÃ©tricas actualizadas sin latencia de polling tradicional.

#### **ğŸ”§ CaracterÃ­sticas tÃ©cnicas:**

##### **ğŸ“Š ConfiguraciÃ³n Socket.io:**
- **Framework**: Node.js + Express + Socket.io
- **Transports**: WebSocket + HTTP Long-polling (fallback)
- **CORS**: Configurado para Cloud Run domains
- **Conexiones**: MÃºltiples clientes simultÃ¡neos
- **Persistence**: Mantiene estado de conexiones activas

##### **ğŸ—„ï¸ IntegraciÃ³n con Cloud SQL:**
- **Connection Pool**: Pool optimizado de conexiones MySQL
- **Query optimizada**: SELECT con LIMIT 1 ORDER BY created_at DESC
- **Polling automÃ¡tico**: VerificaciÃ³n cada 2 segundos de nuevos datos
- **Error handling**: ReconexiÃ³n automÃ¡tica en caso de fallos DB

##### **ğŸŒ ConfiguraciÃ³n CORS avanzada:**
```javascript
cors: {
  origin: [
    "http://localhost:5173",                    // Desarrollo local
    "https://sopes1-frontend-realtime-*.run.app", // Cloud Run dinÃ¡mico
    /^https:\/\/.*\.run\.app$/                  // Regex para dominios Cloud Run
  ],
  methods: ["GET", "POST"],
  credentials: true
}
```

### ğŸ“ˆ Endpoints y Funcionalidades

#### **ğŸ”— WebSocket Events:**

##### **`connection` - Nueva ConexiÃ³n**
- **Trigger**: Cliente se conecta al servidor
- **AcciÃ³n**: 
  - Log de nueva conexiÃ³n con ID Ãºnico
  - EnvÃ­o inmediato de datos mÃ¡s recientes
  - Registro en lista de clientes activos
- **Datos enviados**: Ãšltima mÃ©trica completa de Cloud SQL

##### **`metrics_update` - ActualizaciÃ³n de MÃ©tricas**
- **Trigger**: Datos nuevos detectados en base de datos
- **Frecuencia**: Cada 2 segundos (configurable)
- **Payload**: JSON estructurado con mÃ©tricas completas
- **Broadcasting**: EnvÃ­o simultÃ¡neo a todos los clientes conectados

##### **`disconnect` - DesconexiÃ³n**
- **Trigger**: Cliente cierra conexiÃ³n
- **AcciÃ³n**: 
  - Log de desconexiÃ³n
  - Limpieza de recursos asociados
  - ActualizaciÃ³n de contador de clientes

#### **ğŸ”§ REST Endpoints de soporte:**

##### **`GET /health` - Health Check**
- **FunciÃ³n**: VerificaciÃ³n de estado del servicio Socket.io
- **Respuesta**: Estado + contador de conexiones activas
- **Uso**: Kubernetes health probes y monitoreo

##### **`GET /api/metrics/latest` - Endpoint de respaldo**
- **FunciÃ³n**: API REST tradicional para obtener datos
- **Uso**: Fallback en caso de problemas WebSocket
- **Respuesta**: Mismos datos que Socket.io pero via HTTP

### ğŸ”„ Flujo de Datos en Tiempo Real

#### **ğŸ“Š Estructura de datos transmitidos:**
```json
{
  "cpu": 48,
  "ram": 93,
  "ram_details": {
    "total": 3913,
    "used": 3648,
    "free": 265
  },
  "processes": {
    "total_procesos": 146,
    "procesos_corriendo": 16,
    "procesos_durmiendo": 128,
    "procesos_zombies": 2,
    "procesos_parados": 0
  },
  "data_source": "nodejs_api1",
  "timestamp": "2025-06-26T05:34:01.000Z"
}
```

#### **â±ï¸ Ciclo de actualizaciÃ³n automÃ¡tica:**
1. **Query a Cloud SQL** â†’ Obtener Ãºltimo registro cada 2s
2. **ComparaciÃ³n de datos** â†’ Verificar si hay cambios
3. **Broadcasting** â†’ Enviar a todos los clientes conectados
4. **Logging** â†’ Registrar actividad de transmisiÃ³n
5. **Repetir ciclo** â†’ Proceso continuo automÃ¡tico

### ğŸŒ IntegraciÃ³n Frontend (Cloud Run)

#### **ğŸ”— Cliente Socket.io en React:**

##### **ğŸ“¡ ConfiguraciÃ³n de conexiÃ³n:**
- **URL**: NodePort directo del cluster (bypass ingress)
- **Transports**: WebSocket preferido, polling como fallback
- **Auto-reconnect**: ReconexiÃ³n automÃ¡tica en caso de desconexiÃ³n
- **Event handlers**: Manejo de conexiÃ³n, datos y errores

##### **âš™ï¸ Funcionalidades del cliente:**
- **ConexiÃ³n automÃ¡tica**: Al cargar el dashboard
- **Callback registration**: FunciÃ³n para actualizar UI
- **Error handling**: Manejo graceful de errores de conexiÃ³n
- **DesconexiÃ³n limpia**: Cleanup al salir del componente

##### **ğŸ¯ Ventajas sobre polling tradicional:**
- **Latencia reducida**: Actualizaciones instantÃ¡neas sin delay
- **Eficiencia de red**: Sin requests HTTP constantes
- **Escalabilidad**: Una conexiÃ³n por cliente vs mÃºltiples requests
- **Experiencia de usuario**: UI mÃ¡s fluida y responsiva

### ğŸ”§ Despliegue y ConfiguraciÃ³n

#### **ğŸ³ Deployment en Kubernetes:**
- **Imagen**: `josue013/sopes1-nodejs-realtime-api:latest`
- **Puerto**: 4000 (interno del pod)
- **Service**: NodePort 30080 (acceso externo directo)
- **RÃ©plicas**: 1 (optimizado para persistencia de conexiones)
- **Resources**: 256Mi RAM, 0.25 CPU

#### **ğŸŒ Acceso externo:**
- **Tipo**: NodePort (bypass del ingress para WebSockets)
- **Puerto externo**: 30080 (fijo en todos los nodos)
- **Protocolo**: HTTP/WebSocket (con upgrade automÃ¡tico)
- **Load balancing**: Directo a pod especÃ­fico

#### **ğŸ”— Variables de entorno:**
```bash
DB_HOST=34.61.65.213      # Cloud SQL IP privada
DB_USER=root              # Usuario MySQL
DB_PASSWORD=***           # Password seguro
DB_NAME=sopes1_db         # Base de datos del proyecto
PORT=4000                # Puerto interno del servicio
```

### ğŸ“Š Monitoreo y Observabilidad

#### **ğŸ“ˆ MÃ©tricas de Socket.io:**
- **Conexiones activas**: Contador en tiempo real
- **Messages per second**: Throughput de mensajes
- **Latencia de conexiÃ³n**: Tiempo de establecimiento
- **Error rate**: Porcentaje de conexiones fallidas

#### **ğŸ” Logging detallado:**
```
ğŸš€ Socket.io API corriendo en puerto: 4000
ğŸŸ¢ Cliente conectado: abc123-def456
ğŸ“Š Datos iniciales enviados a: abc123-def456
ğŸ“¡ Broadcasting datos a todos los clientes
ğŸ”´ Cliente desconectado: abc123-def456
```

#### **âš ï¸ Health checks automÃ¡ticos:**
- **Kubernetes probes**: VerificaciÃ³n de estado del pod
- **Connection counting**: Monitoreo de clientes activos
- **Database connectivity**: VerificaciÃ³n de conexiÃ³n a Cloud SQL
- **Memory usage**: Control de uso de recursos

### ğŸ¯ Ventajas de la ImplementaciÃ³n

#### **ğŸš€ Performance:**
- **Latencia ultra-baja**: <100ms entre dato nuevo y UI
- **Eficiencia de red**: 90% menos trÃ¡fico vs polling
- **Escalabilidad**: Soporte para cientos de clientes simultÃ¡neos
- **Resource optimization**: Menor uso de CPU y memoria

#### **ğŸ‘¤ Experiencia de Usuario:**
- **Tiempo real genuino**: Sin retrasos perceptibles
- **UI responsiva**: Actualizaciones fluidas sin parpadeos
- **ConexiÃ³n confiable**: ReconexiÃ³n automÃ¡tica transparente
- **Datos siempre frescos**: Sin datos obsoletos

#### **ğŸ”§ Arquitectura:**
- **SeparaciÃ³n de responsabilidades**: API dedicada solo a tiempo real
- **Escalabilidad independiente**: Escalar segÃºn demanda WebSocket
- **Fault tolerance**: Fallback a REST API si WebSocket falla
- **Cloud-native**: IntegraciÃ³n nativa con Cloud Run y GKE

Socket.io en Fase 2 representa la **evoluciÃ³n hacia arquitecturas de tiempo real**, transformando la experiencia tradicional de dashboards estÃ¡ticos hacia **interfaces dinÃ¡micas y responsivas** que reflejan el estado del sistema instantÃ¡neamente, proporcionando una **ventaja competitiva significativa** en observabilidad y monitoreo de sistemas.


## <a name="csql">8. Cloud SQL

### ğŸ“‹ Objetivo de Cloud SQL en Fase 2

Cloud SQL implementa una **base de datos MySQL totalmente administrada** por Google Cloud Platform, reemplazando la instancia local de MySQL de la Fase 1. El objetivo es proporcionar **alta disponibilidad**, **escalabilidad automÃ¡tica** y **administraciÃ³n simplificada** de la base de datos, eliminando la necesidad de gestiÃ³n manual de infraestructura.

### ğŸ—ï¸ EvoluciÃ³n desde Fase 1

| **Aspecto**              | **Fase 1**                    | **Fase 2**                           |
|--------------------------|-------------------------------|--------------------------------------|
| **Tipo de servicio**    | MySQL en contenedor Docker    | Cloud SQL MySQL administrado        |
| **AdministraciÃ³n**      | Manual (configuraciÃ³n, backups) | AutomÃ¡tica (Google Cloud)           |
| **Escalabilidad**       | Limitada por recursos locales | Escalado automÃ¡tico bajo demanda    |
| **Alta disponibilidad** | Single point of failure       | ReplicaciÃ³n automÃ¡tica multi-zona   |
| **Backups**             | Manuales o scripts custom      | Backups automÃ¡ticos diarios         |
| **Monitoreo**           | Logs bÃ¡sicos                   | MÃ©tricas integradas con GCP          |
| **Seguridad**           | ConfiguraciÃ³n manual           | Cifrado automÃ¡tico y polÃ­ticas IAM  |

### ğŸ”§ ConfiguraciÃ³n de la Instancia

#### **ğŸ“Š Especificaciones:**
- **Tipo**: Cloud SQL for MySQL 8.0
- **Instancia**: `db-n1-standard-1` (1 vCPU, 3.75 GB RAM)
- **Almacenamiento**: 20 GB SSD con auto-incremento habilitado
- **RegiÃ³n**: `us-central1` (mismo que el cluster GKE)
- **Zona**: Multi-zona para alta disponibilidad
- **IP PÃºblica**: `34.61.65.213`

#### **ğŸ›¡ï¸ ConfiguraciÃ³n de seguridad:**
- **Conexiones autorizadas**: Acceso desde IPs especÃ­ficas y servicios GCP
- **SSL**: Conexiones cifradas obligatorias
- **Usuario root**: Configurado con password seguro
- **Backup automÃ¡tico**: Habilitado con retenciÃ³n de 7 dÃ­as
- **Maintenance window**: Configurado para horarios de baja actividad

### ğŸ—„ï¸ Estructura de la Base de Datos

#### **ğŸ›ï¸ Base de datos:** `sopes1_db`

La base de datos implementa una **tabla unificada** que consolida todas las mÃ©tricas del sistema en una sola estructura optimizada:

#### **ğŸ“Š Tabla `system_metrics` (Unificada):**

```sql
CREATE TABLE IF NOT EXISTS system_metrics (
    id INT AUTO_INCREMENT PRIMARY KEY,
    ram_total BIGINT,
    ram_free BIGINT,
    ram_used BIGINT,
    ram_percentage FLOAT,
    cpu_usage FLOAT,
    cpu_free FLOAT,
    running_processes INT,
    total_processes INT,
    sleeping_processes INT,
    zombie_processes INT,
    stopped_processes INT,
    data_source VARCHAR(50),
    created_at DATETIME DEFAULT CURRENT_TIMESTAMP
);
```

#### **ğŸ”§ Ventajas de la tabla unificada:**

##### **ğŸ“ˆ Eficiencia de consultas:**
- **Single query**: Todas las mÃ©tricas en una sola consulta
- **Joins eliminados**: No necesidad de unir mÃºltiples tablas
- **Performance optimizada**: Ãndices optimizados para consultas temporales
- **Atomicidad**: Cada registro representa un snapshot completo del sistema

##### **ğŸ” Facilidad de anÃ¡lisis:**
- **CorrelaciÃ³n temporal**: Todas las mÃ©tricas tienen el mismo timestamp
- **AnÃ¡lisis integral**: VisualizaciÃ³n completa del estado del sistema
- **Trending simplificado**: AnÃ¡lisis de tendencias mÃ¡s directo
- **Reporting unificado**: Reportes consolidados sin complejidad

### ğŸ“Š Campos y Tipos de Datos

| **Campo**              | **Tipo**        | **DescripciÃ³n**                           |
|------------------------|-----------------|-------------------------------------------|
| `id`                   | `INT PK AUTO`   | Identificador Ãºnico autoincremental      |
| `ram_total`            | `BIGINT`        | Memoria RAM total en MB                   |
| `ram_free`             | `BIGINT`        | Memoria RAM libre en MB                   |
| `ram_used`             | `BIGINT`        | Memoria RAM usada en MB                   |
| `ram_percentage`       | `FLOAT`         | Porcentaje de uso de RAM                  |
| `cpu_usage`            | `FLOAT`         | Porcentaje de uso de CPU                  |
| `cpu_free`             | `FLOAT`         | Porcentaje libre de CPU                   |
| `running_processes`    | `INT`           | NÃºmero de procesos en ejecuciÃ³n          |
| `total_processes`      | `INT`           | Total de procesos en el sistema          |
| `sleeping_processes`   | `INT`           | Procesos en estado durmiendo             |
| `zombie_processes`     | `INT`           | Procesos zombie                          |
| `stopped_processes`    | `INT`           | Procesos parados                         |
| `data_source`          | `VARCHAR(50)`   | API que insertÃ³ el registro              |
| `created_at`           | `DATETIME`      | Timestamp de inserciÃ³n automÃ¡tico       |

### ğŸ”— Conectividad desde Kubernetes

#### **ğŸ”§ Variables de entorno:**
```bash
DB_HOST=34.61.65.213      # IP pÃºblica de Cloud SQL
DB_USER=root              # Usuario de base de datos
DB_PASSWORD=***           # Password seguro generado
DB_NAME=sopes1_db         # Base de datos del proyecto
DB_PORT=3306              # Puerto MySQL estÃ¡ndar
```

#### **ğŸ ConexiÃ³n desde Python API:**
```python
import pymysql

DB_CONFIG = {
    'host': '34.61.65.213',
    'user': 'root',
    'password': os.getenv('DB_PASSWORD'),
    'database': 'sopes1_db',
    'port': 3306,
    'charset': 'utf8mb4'
}

connection = pymysql.connect(**DB_CONFIG)
```

#### **ğŸŸ¢ ConexiÃ³n desde Node.js APIs:**
```javascript
const mysql = require('mysql2/promise');

const pool = mysql.createPool({
    host: '34.61.65.213',
    user: 'root',
    password: process.env.DB_PASSWORD,
    database: 'sopes1_db',
    port: 3306,
    connectionLimit: 10
});
```




### ğŸ¯ Ventajas de Cloud SQL

#### **ğŸš€ Operacionales:**
- **AdministraciÃ³n cero**: Google gestiona updates, patches y maintenance
- **Escalabilidad automÃ¡tica**: Incremento de recursos bajo demanda
- **Alta disponibilidad**: 99.95% SLA con failover automÃ¡tico
- **Backups automÃ¡ticos**: Respaldos diarios con point-in-time recovery

#### **ğŸ›¡ï¸ Seguridad:**
- **Cifrado en reposo**: Datos automÃ¡ticamente cifrados
- **Cifrado en trÃ¡nsito**: Conexiones SSL/TLS obligatorias
- **IAM integrado**: Control de acceso con polÃ­ticas de Google Cloud
- **Audit logs**: Registro completo de actividades

#### **ğŸ’° EconÃ³micas:**
- **Pay-as-you-use**: Pago por recursos consumidos
- **No upfront costs**: Sin inversiÃ³n inicial en hardware
- **OptimizaciÃ³n automÃ¡tica**: Ajuste automÃ¡tico de recursos
- **Reduced operational overhead**: Menor costo de administraciÃ³n

### ğŸ”„ IntegraciÃ³n con el Ecosistema

#### **ğŸ“¡ Flujo de datos:**
```
VM (MÃ³dulos) â†’ APIs (Python/Node.js) â†’ Cloud SQL â†’ Socket.io â†’ Frontend
      â†“                â†“                    â†“           â†“          â†“
   MÃ©tricas      Procesamiento        Persistencia   Tiempo Real  VisualizaciÃ³n
```

#### **ğŸ¯ Casos de uso soportados:**
- **InserciÃ³n masiva**: Miles de registros por minuto via APIs
- **Consultas en tiempo real**: Socket.io obteniendo datos cada 2 segundos
- **AnÃ¡lisis histÃ³rico**: Dashboard con tendencias y estadÃ­sticas
- **Monitoreo de salud**: Health checks de todas las APIs

Cloud SQL en Fase 2 representa la **transformaciÃ³n hacia servicios administrados**, eliminando la complejidad operacional de la gestiÃ³n de bases de datos mientras proporciona **enterprise-grade reliability** y **performance optimizada** para aplicaciones cloud-native modernas.

## <a name="crun">9. Cloud Run

### ğŸ“‹ Objetivo de Cloud Run en Fase 2

Cloud Run implementa un **servicio serverless completamente administrado** para el despliegue del frontend React. El objetivo es proporcionar una **plataforma de hosting escalable** que se ajuste automÃ¡ticamente a la demanda sin necesidad de gestiÃ³n de infraestructura.

### ğŸš€ CaracterÃ­sticas del servicio

#### **ğŸ³ Despliegue de contenedor:**
- **Imagen Docker**: Frontend React empaquetado en contenedor
- **Puerto interno**: 5173 (puerto estÃ¡ndar de desarrollo Vite/React)
- **Protocolo**: HTTP/HTTPS automÃ¡tico
- **Escalado**: AutomÃ¡tico de 0 a N instancias segÃºn demanda

### ğŸ”§ ConfiguraciÃ³n del servicio

#### **ğŸ“Š Especificaciones tÃ©cnicas:**
- **CPU**: 4 vCPU por instancia
- **Memoria**: 2gb por instancia
- **Concurrencia**: 80 requests por instancia
- **Timeout**: 300 segundos

#### **ğŸŒ Conectividad:**
- **URL pÃºblica**: Generada automÃ¡ticamente por Cloud Run
- **HTTPS**: Certificado SSL/TLS automÃ¡tico
- **Dominio personalizado**: Configurable si se requiere
- **Acceso**: PÃºblico (sin autenticaciÃ³n requerida)

### ğŸ”— IntegraciÃ³n con el ecosistema

#### **ğŸ“¡ Conexiones del frontend:**
- **Socket.io**: ConexiÃ³n directa al NodePort del cluster GKE (puerto 30080)
- **WebSockets**: ComunicaciÃ³n en tiempo real para actualizaciones de mÃ©tricas
- **CORS**: Configurado en todas las APIs para permitir requests desde Cloud Run

### ğŸ¯ Ventajas de Cloud Run

#### **ğŸš€ Serverless:**
- **Escalado automÃ¡tico**: De 0 a miles de instancias automÃ¡ticamente
- **Pago por uso**: Solo se cobra por requests procesados
- **GestiÃ³n cero**: Google administra toda la infraestructura subyacente
- **Cold start**: InicializaciÃ³n rÃ¡pida de nuevas instancias.

Cloud Run en Fase 2 proporciona una **soluciÃ³n serverless moderna** para el frontend, eliminando la complejidad de gestiÃ³n de servidores mientras ofrece **escalabilidad automÃ¡tica** y **alta disponibilidad** para la interfaz de usuario del sistema de monitoreo.

## ğŸ‘¨â€ğŸ’» Autor

[@Josue013](https://www.github.com/Josue013)